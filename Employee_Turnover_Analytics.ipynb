{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivashankariramanimohan/employee_turnover_analysis/blob/main/Employee_Turnover_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Employee Turnover Analytics - Complete Solution\n",
        "# Portobello Tech - ML Course-End Project"
      ],
      "metadata": {
        "id": "mX5bnsjP1O-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.cluster import KMeans\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I55TXqpTBYFy",
        "outputId": "9e2092eb-1fba-4042-f3c1-b10efda33665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: DATA LOADING AND QUALITY CHECKS"
      ],
      "metadata": {
        "id": "ez-7FfheBhC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data_path = '/content/drive/My Drive/Simplilearn_project/HR_comma_sep.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nDataset Description:\")\n",
        "print(df.describe())\n",
        "\n",
        "# 1. Data Quality Checks - Missing Values\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 1: DATA QUALITY CHECKS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nMissing Values Check:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "if missing_values.sum() == 0:\n",
        "    print(\"✓ No missing values found in the dataset!\")\n",
        "else:\n",
        "    print(\"⚠ Missing values detected!\")\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"\\nDuplicate rows: {duplicates}\")\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"\\nBasic Dataset Statistics:\")\n",
        "print(f\"Total employees: {len(df)}\")\n",
        "print(f\"Employees who left: {df['left'].sum()}\")\n",
        "print(f\"Employees who stayed: {len(df) - df['left'].sum()}\")\n",
        "print(f\"Turnover rate: {df['left'].mean():.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJsWTBzbBlef",
        "outputId": "d4e98e04-6ad2-4438-d400-d6811207e8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "Dataset shape: (14999, 10)\n",
            "\n",
            "First 5 rows:\n",
            "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
            "0                0.38             0.53               2                   157   \n",
            "1                0.80             0.86               5                   262   \n",
            "2                0.11             0.88               7                   272   \n",
            "3                0.72             0.87               5                   223   \n",
            "4                0.37             0.52               2                   159   \n",
            "\n",
            "   time_spend_company  Work_accident  left  promotion_last_5years  sales  \\\n",
            "0                   3              0     1                      0  sales   \n",
            "1                   6              0     1                      0  sales   \n",
            "2                   4              0     1                      0  sales   \n",
            "3                   5              0     1                      0  sales   \n",
            "4                   3              0     1                      0  sales   \n",
            "\n",
            "   salary  \n",
            "0     low  \n",
            "1  medium  \n",
            "2  medium  \n",
            "3     low  \n",
            "4     low  \n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14999 entries, 0 to 14998\n",
            "Data columns (total 10 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   satisfaction_level     14999 non-null  float64\n",
            " 1   last_evaluation        14999 non-null  float64\n",
            " 2   number_project         14999 non-null  int64  \n",
            " 3   average_montly_hours   14999 non-null  int64  \n",
            " 4   time_spend_company     14999 non-null  int64  \n",
            " 5   Work_accident          14999 non-null  int64  \n",
            " 6   left                   14999 non-null  int64  \n",
            " 7   promotion_last_5years  14999 non-null  int64  \n",
            " 8   sales                  14999 non-null  object \n",
            " 9   salary                 14999 non-null  object \n",
            "dtypes: float64(2), int64(6), object(2)\n",
            "memory usage: 1.1+ MB\n",
            "None\n",
            "\n",
            "Dataset Description:\n",
            "       satisfaction_level  last_evaluation  number_project  \\\n",
            "count        14999.000000     14999.000000    14999.000000   \n",
            "mean             0.612834         0.716102        3.803054   \n",
            "std              0.248631         0.171169        1.232592   \n",
            "min              0.090000         0.360000        2.000000   \n",
            "25%              0.440000         0.560000        3.000000   \n",
            "50%              0.640000         0.720000        4.000000   \n",
            "75%              0.820000         0.870000        5.000000   \n",
            "max              1.000000         1.000000        7.000000   \n",
            "\n",
            "       average_montly_hours  time_spend_company  Work_accident          left  \\\n",
            "count          14999.000000        14999.000000   14999.000000  14999.000000   \n",
            "mean             201.050337            3.498233       0.144610      0.238083   \n",
            "std               49.943099            1.460136       0.351719      0.425924   \n",
            "min               96.000000            2.000000       0.000000      0.000000   \n",
            "25%              156.000000            3.000000       0.000000      0.000000   \n",
            "50%              200.000000            3.000000       0.000000      0.000000   \n",
            "75%              245.000000            4.000000       0.000000      0.000000   \n",
            "max              310.000000           10.000000       1.000000      1.000000   \n",
            "\n",
            "       promotion_last_5years  \n",
            "count           14999.000000  \n",
            "mean                0.021268  \n",
            "std                 0.144281  \n",
            "min                 0.000000  \n",
            "25%                 0.000000  \n",
            "50%                 0.000000  \n",
            "75%                 0.000000  \n",
            "max                 1.000000  \n",
            "\n",
            "==================================================\n",
            "STEP 1: DATA QUALITY CHECKS\n",
            "==================================================\n",
            "\n",
            "Missing Values Check:\n",
            "satisfaction_level       0\n",
            "last_evaluation          0\n",
            "number_project           0\n",
            "average_montly_hours     0\n",
            "time_spend_company       0\n",
            "Work_accident            0\n",
            "left                     0\n",
            "promotion_last_5years    0\n",
            "sales                    0\n",
            "salary                   0\n",
            "dtype: int64\n",
            "✓ No missing values found in the dataset!\n",
            "\n",
            "Duplicate rows: 3008\n",
            "\n",
            "Basic Dataset Statistics:\n",
            "Total employees: 14999\n",
            "Employees who left: 3571\n",
            "Employees who stayed: 11428\n",
            "Turnover rate: 23.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: EXPLORATORY DATA ANALYSIS (EDA)"
      ],
      "metadata": {
        "id": "7Xnh99IBDbx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 2: EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 2.1 Correlation Heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5)\n",
        "plt.title('Correlation Matrix Heatmap - Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "e('Distribution of Employee Last Evaluation')\n",
        "axes[1].set_xlabel('Last Evaluation Score')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "# Average Monthly Hours# 2.2 Distribution Plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Employee Satisfaction Distribution\n",
        "axes[0].hist(df['satisfaction_level'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0].set_title('Distribution of Employee Satisfaction Level')\n",
        "axes[0].set_xlabel('Satisfaction Level')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# Employee Evaluation Distribution\n",
        "axes[1].hist(df['last_evaluation'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[1].set_title Distribution\n",
        "axes[2].hist(df['average_montly_hours'], bins=30, alpha=0.7, color='salmon', edgecolor='black')\n",
        "axes[2].set_title('Distribution of Average Monthly Hours')\n",
        "axes[2].set_xlabel('Average Monthly Hours')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2.3 Bar Plot - Project Count by Employee Status\n",
        "plt.figure(figsize=(10, 6))\n",
        "project_left = df.groupby(['number_project', 'left']).size().unstack()\n",
        "project_left.plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "plt.title('Employee Project Count Distribution (Left vs Stayed)')\n",
        "plt.xlabel('Number of Projects')\n",
        "plt.ylabel('Number of Employees')\n",
        "plt.legend(['Stayed', 'Left'])\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Insights from project count analysis\n",
        "print(\"\\nInsights from Project Count Analysis:\")\n",
        "project_analysis = df.groupby('number_project')['left'].agg(['count', 'sum', 'mean'])\n",
        "project_analysis.columns = ['Total_Employees', 'Employees_Left', 'Turnover_Rate']\n",
        "print(project_analysis)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "8tExUb2jIED3",
        "outputId": "79881b72-5660-476f-b700-ca59a97a6a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-8-1926626817>, line 30)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-1926626817>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    axes[1].set_titl Distribution\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: CLUSTERING ANALYSIS"
      ],
      "metadata": {
        "id": "pNlmgaU-D2Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 3: CLUSTERING ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 3.1 & 3.2 K-means Clustering for employees who left\n",
        "left_employees = df[df['left'] == 1][['satisfaction_level', 'last_evaluation']].copy()\n",
        "print(f\"Number of employees who left: {len(left_employees)}\")\n",
        "\n",
        "# Perform K-means clustering with 3 clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "left_employees['cluster'] = kmeans.fit_predict(left_employees)\n",
        "\n",
        "# Visualize clusters\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['red', 'blue', 'green']\n",
        "for i in range(3):\n",
        "    cluster_data = left_employees[left_employees['cluster'] == i]\n",
        "    plt.scatter(cluster_data['satisfaction_level'], cluster_data['last_evaluation'],\n",
        "                c=colors[i], label=f'Cluster {i}', alpha=0.6)\n",
        "\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='black', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "plt.xlabel('Satisfaction Level')\n",
        "plt.ylabel('Last Evaluation')\n",
        "plt.title('K-means Clustering of Employees Who Left (3 Clusters)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 3.3 Cluster Analysis\n",
        "print(\"\\nCluster Analysis for Employees Who Left:\")\n",
        "cluster_stats = left_employees.groupby('cluster').agg({\n",
        "    'satisfaction_level': ['mean', 'std'],\n",
        "    'last_evaluation': ['mean', 'std']\n",
        "}).round(3)\n",
        "print(cluster_stats)\n",
        "\n",
        "print(\"\\nCluster Interpretations:\")\n",
        "for i in range(3):\n",
        "    cluster_data = left_employees[left_employees['cluster'] == i]\n",
        "    avg_satisfaction = cluster_data['satisfaction_level'].mean()\n",
        "    avg_evaluation = cluster_data['last_evaluation'].mean()\n",
        "    print(f\"\\nCluster {i} ({len(cluster_data)} employees):\")\n",
        "    print(f\"  - Average Satisfaction: {avg_satisfaction:.3f}\")\n",
        "    print(f\"  - Average Evaluation: {avg_evaluation:.3f}\")\n",
        "\n",
        "    if avg_satisfaction < 0.5 and avg_evaluation < 0.6:\n",
        "        print(\"  - Profile: Low performers with low satisfaction\")\n",
        "    elif avg_satisfaction < 0.5 and avg_evaluation > 0.7:\n",
        "        print(\"  - Profile: High performers with low satisfaction (burned out)\")\n",
        "    else:\n",
        "        print(\"  - Profile: Mixed performance and satisfaction levels\")\n"
      ],
      "metadata": {
        "id": "nV-s6DZRIPoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4: DATA PREPROCESSING AND CLASS IMBALANCE HANDLING"
      ],
      "metadata": {
        "id": "uTow5HbuE-aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 4: DATA PREPROCESSING AND SMOTE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 4.1 Preprocessing - Convert categorical to numerical\n",
        "df_processed = df.copy()\n",
        "\n",
        "# First, let's check the actual column names\n",
        "print(\"Actual column names in dataset:\")\n",
        "print(df_processed.columns.tolist())\n",
        "\n",
        "# Identify categorical and numerical columns based on actual data\n",
        "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"\\nCategorical columns: {categorical_cols}\")\n",
        "print(f\"Numerical columns: {numerical_cols}\")\n",
        "\n",
        "# Apply get_dummies to categorical variables\n",
        "if len(categorical_cols) > 0:\n",
        "    df_categorical = pd.get_dummies(df_processed[categorical_cols], prefix=categorical_cols)\n",
        "    df_numerical = df_processed[numerical_cols]\n",
        "else:\n",
        "    # If no categorical columns detected, check for specific columns that should be categorical\n",
        "    potential_categorical = []\n",
        "    for col in df_processed.columns:\n",
        "        if col in ['sales', 'department', 'Department', 'salary'] or df_processed[col].dtype == 'object':\n",
        "            potential_categorical.append(col)\n",
        "\n",
        "    if potential_categorical:\n",
        "        print(f\"Found potential categorical columns: {potential_categorical}\")\n",
        "        df_categorical = pd.get_dummies(df_processed[potential_categorical], prefix=potential_categorical)\n",
        "        df_numerical = df_processed[[col for col in df_processed.columns if col not in potential_categorical]]\n",
        "    else:\n",
        "        print(\"No categorical columns found. Using all columns as numerical.\")\n",
        "        df_categorical = pd.DataFrame()  # Empty dataframe\n",
        "        df_numerical = df_processed\n",
        "\n",
        "df_final = pd.concat([df_numerical, df_categorical], axis=1)\n",
        "else:\n",
        "    df_final = df_numerical.copy()\n",
        "print(f\"\\nFinal # Combine categorical and numerical variables\n",
        "if not df_categorical.empty:\n",
        "    dataset shape after preprocessing: {df_final.shape}\")\n",
        "print(f\"Final columns: {df_final.columns.tolist()}\")\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_final.drop('left', axis=1)\n",
        "y = df_final['left']\n",
        "\n",
        "print(f\"\\nClass distribution before SMOTE:\")\n",
        "print(f\"Class 0 (Stayed): {sum(y == 0)}\")\n",
        "print(f\"Class 1 (Left): {sum(y == 1)}\")\n",
        "\n",
        "# 4.2 Stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=123, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# 4.3 Apply SMOTE to training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"\\nClass distribution after SMOTE:\")\n",
        "print(f\"Class 0 (Stayed): {sum(y_train_smote == 0)}\")\n",
        "print(f\"Class 1 (Left): {sum(y_train_smote == 1)}\")\n"
      ],
      "metadata": {
        "id": "yOUV83QGIV6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5: MODEL TRAINING AND CROSS-VALIDATION"
      ],
      "metadata": {
        "id": "KOR2tOtSIc7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 5: MODEL TRAINING AND CROSS-VALIDATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
        "}\n",
        "\n",
        "# 5-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "\n",
        "    # Fit model on SMOTE data\n",
        "    model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "    # Cross-validation scores\n",
        "    cv_scores = cross_val_score(model, X_train_smote, y_train_smote, cv=cv, scoring='accuracy')\n",
        "    cv_results[name] = cv_scores\n",
        "\n",
        "    print(f\"{name} CV Scores: {cv_scores}\")\n",
        "    print(f\"{name} Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "    # Predictions on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(f\"\\n{name} Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "s81Y3pwTIeIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6: MODEL EVALUATION AND BEST MODEL SELECTION"
      ],
      "metadata": {
        "id": "oItO8BsdIpVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 6: MODEL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train models and get predictions\n",
        "model_results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Train model\n",
        "    model.fit(X_train_smote, y_train_smote)\n",
        "    trained_models[name] = model\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    model_results[name] = {\n",
        "        'predictions': y_pred,\n",
        "        'probabilities': y_pred_proba,\n",
        "        'auc_score': auc_score,\n",
        "        'confusion_matrix': conf_matrix\n",
        "    }\n",
        "\n",
        "# 6.1 ROC Curves\n",
        "plt.figure(figsize=(12, 8))\n",
        "for name, results in model_results.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, results['probabilities'])\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {results[\"auc_score\"]:.3f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves for All Models')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 6.2 Confusion Matrices\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for i, (name, results) in enumerate(model_results.items()):\n",
        "    sns.heatmap(results['confusion_matrix'], annot=True, fmt='d',\n",
        "                cmap='Blues', ax=axes[i])\n",
        "    axes[i].set_title(f'{name} - Confusion Matrix')\n",
        "    axes[i].set_xlabel('Predicted')\n",
        "    axes[i].set_ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed metrics\n",
        "print(\"\\nDetailed Model Performance:\")\n",
        "print(\"-\" * 60)\n",
        "for name, results in model_results.items():\n",
        "    tn, fp, fn, tp = results['confusion_matrix'].ravel()\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  AUC Score: {results['auc_score']:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "# 6.3 best Model SelectionB\n",
        "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['auc_score'])\n",
        "best_model = trained_models[best_model_name]\n",
        "best_auc = model_results[best_model_name]['auc_score']\n",
        "\n",
        "print(f\"\\n🏆 BEST MODEL: {best_model_name}\")\n",
        "print(f\"   AUC Score: {best_auc:.4f}\")\n",
        "\n",
        "print(f\"\\n6.3 Evaluation Metric Justification:\")\n",
        "print(\"For employee turnover prediction, RECALL is more important than Precision because:\")\n",
        "print(\"- We want to identify as many employees likely to leave as possible (minimize false negatives)\")\n",
        "print(\"- Missing a potential departure (false negative) is costlier than false alarms (false positive)\")\n",
        "print(\"- HR can afford to implement retention strategies for some false positives\")\n",
        "print(\"- The cost of employee turnover is much higher than the cost of unnecessary retention efforts\")\n"
      ],
      "metadata": {
        "id": "1Pi2PgytItIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 7: RETENTION STRATEGIES"
      ],
      "metadata": {
        "id": "pjRteFv1I4gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 7: RETENTION STRATEGIES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 7.1 Predict probabilities for test set using best model\n",
        "test_probabilities = model_results[best_model_name]['probabilities']\n",
        "\n",
        "# 7.2 Categorize employees into risk zones\n",
        "def categorize_risk(prob):\n",
        "    if prob < 0.2:\n",
        "        return 'Safe Zone (Green)'\n",
        "    elif prob < 0.6:\n",
        "        return 'Low-Risk Zone (Yellow)'\n",
        "    elif prob < 0.9:\n",
        "        return 'Medium-Risk Zone (Orange)'\n",
        "    else:\n",
        "        return 'High-Risk Zone (Red)'\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame({\n",
        "    'Employee_ID': range(len(y_test)),\n",
        "    'Actual_Left': y_test.values,\n",
        "    'Predicted_Probability': test_probabilities,\n",
        "    'Risk_Zone': [categorize_risk(p) for p in test_probabilities]\n",
        "})\n",
        "\n",
        "# Count employees in each zone\n",
        "zone_counts = results_df['Risk_Zone'].value_counts()\n",
        "print(\"Employee Distribution by Risk Zone:\")\n",
        "print(zone_counts)\n",
        "\n",
        "# Visualization of risk zones\n",
        "plt.figure(figsize=(12, 8))\n",
        "zone_colors = {'Safe Zone (Green)': 'green', 'Low-Risk Zone (Yellow)': 'yellow',\n",
        "               'Medium-Risk Zone (Orange)': 'orange', 'High-Risk Zone (Red)': 'red'}\n",
        "\n",
        "for zone in zone_counts.index:\n",
        "    zone_data = results_df[results_df['Risk_Zone'] == zone]\n",
        "    plt.scatter(range(len(zone_data)), zone_data['Predicted_Probability'],\n",
        "                c=zone_colors[zone], label=zone, alpha=0.6)\n",
        "\n",
        "plt.axhline(y=0.2, color='green', linestyle='--', alpha=0.5)\n",
        "plt.axhline(y=0.6, color='yellow', linestyle='--', alpha=0.5)\n",
        "plt.axhline(y=0.9, color='orange', linestyle='--', alpha=0.5)\n",
        "plt.xlabel('Employee Index')\n",
        "plt.ylabel('Turnover Probability')\n",
        "plt.title('Employee Risk Zone Classification')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Retention Strategies\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RETENTION STRATEGIES BY RISK ZONE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "strategies = {\n",
        "    'Safe Zone (Green)': [\n",
        "        \"✅ Maintain current engagement levels\",\n",
        "        \"✅ Regular check-ins to ensure continued satisfaction\",\n",
        "        \"✅ Recognition and appreciation programs\",\n",
        "        \"✅ Career development discussions during annual reviews\"\n",
        "    ],\n",
        "\n",
        "    'Low-Risk Zone (Yellow)': [\n",
        "        \"⚠️ Quarterly one-on-one meetings with managers\",\n",
        "        \"⚠️ Skills development and training opportunities\",\n",
        "        \"⚠️ Flexible work arrangements consideration\",\n",
        "        \"⚠️ Team building activities and social events\",\n",
        "        \"⚠️ Monitor workload and project assignments\"\n",
        "    ],\n",
        "\n",
        "    'Medium-Risk Zone (Orange)': [\n",
        "        \"🔶 Immediate manager intervention and support\",\n",
        "        \"🔶 Comprehensive career planning sessions\",\n",
        "        \"🔶 Salary and benefits review\",\n",
        "        \"🔶 Mentorship program enrollment\",\n",
        "        \"🔶 Workload redistribution if overworked\",\n",
        "        \"🔶 Consider role changes or lateral moves\",\n",
        "        \"🔶 Monthly satisfaction surveys\"\n",
        "    ],\n",
        "\n",
        "    'High-Risk Zone (Red)': [\n",
        "        \"🚨 URGENT: Senior management involvement\",\n",
        "        \"🚨 Exit interview preparation to understand concerns\",\n",
        "        \"🚨 Immediate salary and benefits adjustment consideration\",\n",
        "        \"🚨 Special project assignments to re-engage\",\n",
        "        \"🚨 Fast-track promotion evaluation\",\n",
        "        \"🚨 Personal development plan with clear milestones\",\n",
        "        \"🚨 Weekly check-ins with HR and management\",\n",
        "        \"🚨 Counter-offer preparation if applicable\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for zone, strategy_list in strategies.items():\n",
        "    count = zone_counts.get(zone, 0)\n",
        "    print(f\"\\n{zone} - {count} employees\")\n",
        "    print(\"-\" * 40)\n",
        "    for strategy in strategy_list:\n",
        "        print(f\"  {strategy}\")\n",
        "\n",
        "# Feature importance (for Random Forest or Gradient Boosting)\n",
        "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(f\"FEATURE IMPORTANCE ({best_model_name})\")\n",
        "    print(\"=\"*50)\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = feature_importance.head(10)\n",
        "    plt.barh(range(len(top_features)), top_features['importance'])\n",
        "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title(f'Top 10 Feature Importances - {best_model_name}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"✅ Dataset processed: {df.shape[0]} employees\")\n",
        "print(f\"✅ No missing values detected\")\n",
        "print(f\"✅ Class imbalance handled using SMOTE\")\n",
        "print(f\"✅ 3 models trained and evaluated with 5-fold CV\")\n",
        "print(f\"✅ Best model: {best_model_name} (AUC: {best_auc:.4f})\")\n",
        "print(f\"✅ {len(results_df)} employees categorized into risk zones\")\n",
        "print(f\"✅ Retention strategies provided for all risk categories\")\n",
        "print(\"\\n🎯 Ready for deployment and employee retention implementation!\")"
      ],
      "metadata": {
        "id": "Em8NxZ78I5XA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}